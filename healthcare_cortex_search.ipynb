{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcea3f24",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col\n",
        "from snowflake.snowpark import types as T\n",
        "from snowflake.snowpark import Row\n",
        "from snowflake.core import Root\n",
        "from snowflake.cortex import Complete\n",
        "session = get_active_session()\n",
        "\n",
        "## Need to add snowflake.core and snowflake-ml-python to your packages within your notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f26ea8",
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- View the list of files in the stage to ensure 6 are present\n",
        "LIST @HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE PATTERN='.*\\.pdf$';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34d0755a-992b-4319-99d4-568bbc4b67f7",
      "metadata": {
        "language": "sql",
        "name": "ViewTranscript",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- View a transcript\n",
        "SELECT\n",
        "  SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
        "    '@HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE',\n",
        "    'claim_1.pdf',\n",
        "    OBJECT_CONSTRUCT('mode','Layout')\n",
        "  ) AS layout;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a854a7-cd27-4efa-a687-5b6a525898ad",
      "metadata": {
        "collapsed": false,
        "name": "PARSE_DOCUMENT"
      },
      "source": [
        "### üßæ Extract PDF Transcript Content with `PARSE_DOCUMENT`\n",
        "\n",
        "We‚Äôll use the [`PARSE_DOCUMENT`](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex) function to extract the full contents of each transcript PDF.\n",
        "\n",
        "Snowflake has simplified working with unstructured documents by providing a Cortex AI SQL function that returns the extracted content as a structured JSON object ‚Äî no external parsing tools required.\n",
        "\n",
        "This is the **first step** in working with unstructured data, enabling:\n",
        "- RAG pipelines powered by Cortex Search\n",
        "- LLM workflows like summarization, translation, or classification\n",
        "- Structured output extraction from forms and contracts\n",
        "\n",
        "üí° `PARSE_DOCUMENT` supports two modes:\n",
        "- **OCR mode** ‚Üí Best for clean text extraction from scanned documents  \n",
        "- **LAYOUT mode** ‚Üí Best for preserving structure like tables, headers, and sections (used here)\n",
        "\n",
        "We‚Äôll store the extracted content along with customer name and file path in a `PARSED_TRANSCRIPTS` table for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1860fdc-701c-44b3-83c8-4b0bfc62ee56",
      "metadata": {
        "language": "sql",
        "name": "ParseTranscripts",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE TABLE PARSED_CLAIM_PDFS (\n",
        "    CLAIM_ID NUMBER,\n",
        "    RELATIVE_PATH STRING PRIMARY KEY,\n",
        "    RAW_TEXT VARIANT,\n",
        "    LOADED_AT TIMESTAMP_TZ DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\n",
        "INSERT INTO PARSED_CLAIM_PDFS (CLAIM_ID, RELATIVE_PATH, RAW_TEXT)\n",
        "SELECT \n",
        "    TRY_CAST(REGEXP_REPLACE(RELATIVE_PATH, '.*claim_(\\\\d+)\\\\.pdf$', '\\\\1') AS NUMBER) AS CLAIM_ID,\n",
        "    RELATIVE_PATH,\n",
        "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
        "        '@HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE',\n",
        "        RELATIVE_PATH,\n",
        "        OBJECT_CONSTRUCT('mode','Layout')\n",
        "    ) AS RAW_TEXT\n",
        "FROM DIRECTORY('@HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE') f\n",
        "WHERE \n",
        "  RELATIVE_PATH LIKE 'claim_%.pdf'\n",
        "  AND RELATIVE_PATH NOT IN (SELECT RELATIVE_PATH FROM PARSED_CLAIM_PDFS);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8251a8-8380-4a86-a8b2-d13ea6cef003",
      "metadata": {
        "collapsed": false,
        "name": "SPLIT_TEXT_RECURSIVE_CHARACTER"
      },
      "source": [
        "### ‚úÇÔ∏è Chunk Transcript Text with SPLIT_TEXT_RECURSIVE_CHARACTER\n",
        "Once we‚Äôve extracted the full layout of each transcript, the next step is to split the long text into smaller overlapping chunks. This is a critical step before embedding or indexing for semantic search.\n",
        "\n",
        "We‚Äôll use the [`SPLIT_TEXT_RECURSIVE_CHARACTER`](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex) function, which is designed to split long documents intelligently based on a preferred delimiter (e.g., paragraph breaks).\n",
        "\n",
        "This prepares the data for downstream use with:\n",
        "- Cortex Search indexing\n",
        "- Embedding generation\n",
        "- Summarization or classification with LLMs\n",
        "\n",
        "üí° We‚Äôre using:\n",
        "- A chunk size of **800 characters**\n",
        "- An overlap of **150 characters**\n",
        "- A preferred break on \"\\n\\n\" to preserve paragraph structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be605bb5-63d0-43b1-88ac-6b832aae107f",
      "metadata": {
        "language": "sql",
        "name": "ChunkTranscripts",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Create the chunks table for claims\n",
        "CREATE OR REPLACE TABLE CLAIM_PDF_CHUNKS (\n",
        "    CLAIM_ID NUMBER,\n",
        "    RELATIVE_PATH STRING,\n",
        "    CHUNK STRING\n",
        ");\n",
        "\n",
        "-- Chunk the parsed PDF text using Cortex's split function\n",
        "INSERT INTO CLAIM_PDF_CHUNKS (CLAIM_ID, RELATIVE_PATH, CHUNK)\n",
        "WITH text_chunks AS (\n",
        "    SELECT\n",
        "        CLAIM_ID,\n",
        "        RELATIVE_PATH,\n",
        "        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
        "            RAW_TEXT:content::STRING,  -- extract 'content' field\n",
        "            'markdown',                 -- tokenizer\n",
        "            800,                        -- chunk size\n",
        "            100,                        -- overlap\n",
        "            ARRAY_CONSTRUCT('\\n\\n')     -- preferred break\n",
        "        ) AS CHUNKS\n",
        "    FROM PARSED_CLAIM_PDFS\n",
        "    WHERE RAW_TEXT:content IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        "    CLAIM_ID,\n",
        "    RELATIVE_PATH,\n",
        "    chunk.value::STRING AS CHUNK\n",
        "FROM text_chunks,\n",
        "LATERAL FLATTEN(input => CHUNKS) AS chunk;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249d037c-8049-4b3e-9a25-cd8b3c92077d",
      "metadata": {
        "collapsed": false,
        "name": "CORTEX_SEARCH_SERVICES"
      },
      "source": [
        "### üîç Make Your Transcripts Searchable with Cortex Search\n",
        "\n",
        "Now that we've extracted and chunked the content of each transcript, the next step is to make it **searchable** using Snowflake's fully managed retrieval system: **Cortex Search**.\n",
        "\n",
        "[`CORTEX SEARCH SERVICE`](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) enables **hybrid retrieval** ‚Äî combining vector search (semantic meaning) with keyword search (lexical matching) ‚Äî to deliver **highly accurate results** out of the box.\n",
        "\n",
        "This capability is a key enabler for Retrieval-Augmented Generation (RAG), allowing you to:\n",
        "- Ask contextual questions over your documents using SQL or LLM interfaces\n",
        "- Power chatbots and copilots with document-grounded responses\n",
        "- Serve precise and explainable enterprise search across large volumes of unstructured content\n",
        "\n",
        "üí° Powered by `snowflake-arctic-embed-l-v2.0`, a state-of-the-art embedding model optimized for enterprise search.\n",
        "\n",
        "Once the search service is defined, you can start querying it with natural language ‚Äî directly inside Snowflake ‚Äî without needing external pipelines, vector DBs, or hosting infrastructure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6cee862",
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Create a Cortex Search Service over your chunked transcripts table\n",
        "CREATE OR REPLACE CORTEX SEARCH SERVICE SNOWFLAKE_INTELLIGENCE.AGENTS.CLAIM_PDF_CHUNKS_SEARCH_SERVICE\n",
        "  ON CHUNK\n",
        "  ATTRIBUTES CLAIM_ID, RELATIVE_PATH\n",
        "  WAREHOUSE = SNOWFLAKE_INTELLIGENCE_WH\n",
        "  TARGET_LAG = '365 days'\n",
        "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
        "  AS (\n",
        "    SELECT\n",
        "      CLAIM_ID,\n",
        "      RELATIVE_PATH,\n",
        "      CHUNK::VARCHAR AS CHUNK\n",
        "    FROM SNOWFLAKE_INTELLIGENCE.CONFIG.CLAIM_PDF_CHUNKS\n",
        "  );"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0717015e",
      "metadata": {},
      "source": [
        "### Test your service ‚Äî Baseline **‚Äúone-shot‚Äù RAG** (even with a top-tier model)\n",
        "\n",
        "Select a customer and then run the *BaselineRAGPipeline* cell to perform the **simplest possible Retrieval-Augmented Generation flow**:\n",
        "\n",
        "1. **Retrieve** the **first** transcript chunk that semantically matches the user‚Äôs question.  \n",
        "2. **Generate** an answer by stuffing that single chunk into the prompt of a **state-of-the-art model** (*`claude-3-5-sonnet`* or *`mistral-large2`*).\n",
        "\n",
        "---\n",
        "\n",
        "> Even with a premium LLM you‚Äôll notice it can only repeat whatever facts happen to live in that lone chunk.  \n",
        "> It frequently replies with **‚ÄúBased on the limited context provided ‚Ä¶‚Äù** because:\n",
        ">\n",
        "> * The relevant details might sit in a **different** chunk.  \n",
        "> * We pass **no metadata** (e.g., speaker tags, meeting header) to help the model reason.  \n",
        "> * We don‚Äôt ask it to **extract** or **structure** anything.\n",
        ">\n",
        "> You‚Äôll address all of these gaps in the next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5fc6f1",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Pull distinct CLAIM_IDs from your parsed PDF or chunks table\n",
        "claim_ids = [\n",
        "    r[\"CLAIM_ID\"]\n",
        "    for r in session.table(\"CLAIM_PDF_CHUNKS\")\n",
        "    .select(\"CLAIM_ID\")\n",
        "    .distinct()\n",
        "    .collect()\n",
        "]\n",
        "\n",
        "# Sort for a clean dropdown\n",
        "claim_ids = sorted(claim_ids)\n",
        "\n",
        "# User selects a claim ID to analyze\n",
        "selected = st.selectbox(\n",
        "    \"Run this cell, then select a claim ID to analyze\",\n",
        "    options=claim_ids,\n",
        "    key=\"selected_claim_id\"\n",
        ")\n",
        "\n",
        "st.success(f\"Chosen claim ID: {st.session_state.selected_claim_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c41098",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create service handle\n",
        "root = Root(get_active_session())\n",
        "claim_id = st.session_state.selected_claim_id\n",
        "svc  = root.databases[\"SNOWFLAKE_INTELLIGENCE\"]\\\n",
        "           .schemas[\"AGENTS\"]\\\n",
        "           .cortex_search_services[\"CLAIM_PDF_CHUNKS_SEARCH_SERVICE\"]  # --> Use your Snowflake Cortex Search Service\n",
        "\n",
        "# Specify user question\n",
        "question = f\"\"\"\n",
        "            Provide an explanation of benefits for claim ID {claim_id}.\n",
        "            Include diagnosis, procedure, amount billed, amount paid, patient responsibility,\n",
        "            and a short summary of the visit.\n",
        "            \"\"\".strip()\n",
        "\n",
        "# Get naive retrieval ‚Äì grab the very first hit (could be just the title page)\n",
        "hit = svc.search(\n",
        "        query            = question,\n",
        "        columns          = [\"CLAIM_ID\", \"CHUNK\", \"RELATIVE_PATH\"],\n",
        "        limit            = 5,                # look at a few chunks\n",
        "        search_type      = \"embed\"           # basic semantic search\n",
        "     ).results[0]\n",
        "\n",
        "st.info(f\"**File:** {hit['RELATIVE_PATH']}\\n\\n{hit['CHUNK']}\")\n",
        "\n",
        "# Single-pass generation\n",
        "response = Complete(\n",
        "             model  = \"claude-3-5-sonnet\",  # --> use Anthropic model or Mistral Large model\n",
        "             prompt = (\n",
        "                 f\"{question}\\n\\n\"\n",
        "                 \"Answer **only** using the context below:\\n\"\n",
        "                 \"-----\\n\"\n",
        "                 f\"{hit['CHUNK']}\\n\"\n",
        "                 \"-----\"\n",
        "             )\n",
        "           ).strip()\n",
        "\n",
        "st.info(f\"**LLM Response:**\\n\\n{response}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "lastEditStatus": {
      "authorEmail": "diana.shaw@snowflake.com",
      "authorId": "154296475017",
      "authorName": "DSHAW_SFC",
      "lastEditTime": 1747153634566,
      "notebookId": "uwskyl2ickjotzteiko6",
      "sessionId": "5acc5de6-8e3b-4693-ba67-0103c9ec8cba"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
