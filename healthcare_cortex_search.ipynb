{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcea3f24",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col\n",
        "from snowflake.snowpark import types as T\n",
        "from snowflake.snowpark import Row\n",
        "from snowflake.core import Root\n",
        "from snowflake.cortex import Complete\n",
        "session = get_active_session()\n",
        "\n",
        "## Need to add snowflake.core and snowflake-ml-python to your packages within your notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f26ea8",
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- View the list of files in the stage to ensure 6 are present\n",
        "LIST @HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE PATTERN='.*\\.pdf$';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34d0755a-992b-4319-99d4-568bbc4b67f7",
      "metadata": {
        "language": "sql",
        "name": "ViewTranscript",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- View a transcript\n",
        "SELECT\n",
        "  SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
        "    '@HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE',\n",
        "    'claim_1.pdf',\n",
        "    OBJECT_CONSTRUCT('mode','Layout')\n",
        "  ) AS layout;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a854a7-cd27-4efa-a687-5b6a525898ad",
      "metadata": {
        "collapsed": false,
        "name": "PARSE_DOCUMENT"
      },
      "source": [
        "### ðŸ§¾ Extract PDF Transcript Content with `PARSE_DOCUMENT`\n",
        "\n",
        "Weâ€™ll use the [`PARSE_DOCUMENT`](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex) function to extract the full contents of each transcript PDF.\n",
        "\n",
        "Snowflake has simplified working with unstructured documents by providing a Cortex AI SQL function that returns the extracted content as a structured JSON object â€” no external parsing tools required.\n",
        "\n",
        "This is the **first step** in working with unstructured data, enabling:\n",
        "- RAG pipelines powered by Cortex Search\n",
        "- LLM workflows like summarization, translation, or classification\n",
        "- Structured output extraction from forms and contracts\n",
        "\n",
        "ðŸ’¡ `PARSE_DOCUMENT` supports two modes:\n",
        "- **OCR mode** â†’ Best for clean text extraction from scanned documents  \n",
        "- **LAYOUT mode** â†’ Best for preserving structure like tables, headers, and sections (used here)\n",
        "\n",
        "Weâ€™ll store the extracted content along with customer name and file path in a `PARSED_TRANSCRIPTS` table for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1860fdc-701c-44b3-83c8-4b0bfc62ee56",
      "metadata": {
        "language": "sql",
        "name": "ParseTranscripts",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE TABLE PARSED_CLAIM_PDFS (\n",
        "    CLAIM_ID NUMBER,\n",
        "    RELATIVE_PATH STRING PRIMARY KEY,\n",
        "    RAW_TEXT VARIANT,\n",
        "    LOADED_AT TIMESTAMP_TZ DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\n",
        "INSERT INTO PARSED_CLAIM_PDFS (CLAIM_ID, RELATIVE_PATH, RAW_TEXT)\n",
        "SELECT \n",
        "    TRY_CAST(REGEXP_REPLACE(RELATIVE_PATH, '.*claim_(\\\\d+)\\\\.pdf$', '\\\\1') AS NUMBER) AS CLAIM_ID,\n",
        "    RELATIVE_PATH,\n",
        "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
        "        '@HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE',\n",
        "        RELATIVE_PATH,\n",
        "        OBJECT_CONSTRUCT('mode','Layout')\n",
        "    ) AS RAW_TEXT\n",
        "FROM DIRECTORY('@HEALTHCARE_DEMO_DB.PUBLIC.HEALTHCARE_DEMO_STAGE') f\n",
        "WHERE \n",
        "  RELATIVE_PATH LIKE 'claim_%.pdf'\n",
        "  AND RELATIVE_PATH NOT IN (SELECT RELATIVE_PATH FROM PARSED_CLAIM_PDFS);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8251a8-8380-4a86-a8b2-d13ea6cef003",
      "metadata": {
        "collapsed": false,
        "name": "SPLIT_TEXT_RECURSIVE_CHARACTER"
      },
      "source": [
        "### âœ‚ï¸ Chunk Transcript Text with SPLIT_TEXT_RECURSIVE_CHARACTER\n",
        "Once weâ€™ve extracted the full layout of each transcript, the next step is to split the long text into smaller overlapping chunks. This is a critical step before embedding or indexing for semantic search.\n",
        "\n",
        "Weâ€™ll use the [`SPLIT_TEXT_RECURSIVE_CHARACTER`](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex) function, which is designed to split long documents intelligently based on a preferred delimiter (e.g., paragraph breaks).\n",
        "\n",
        "This prepares the data for downstream use with:\n",
        "- Cortex Search indexing\n",
        "- Embedding generation\n",
        "- Summarization or classification with LLMs\n",
        "\n",
        "ðŸ’¡ Weâ€™re using:\n",
        "- A chunk size of **800 characters**\n",
        "- An overlap of **150 characters**\n",
        "- A preferred break on \"\\n\\n\" to preserve paragraph structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be605bb5-63d0-43b1-88ac-6b832aae107f",
      "metadata": {
        "language": "sql",
        "name": "ChunkTranscripts",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Create the chunks table for claims\n",
        "CREATE OR REPLACE TABLE CLAIM_PDF_CHUNKS (\n",
        "    CLAIM_ID NUMBER,\n",
        "    RELATIVE_PATH STRING,\n",
        "    CHUNK STRING\n",
        ");\n",
        "\n",
        "-- Chunk the parsed PDF text using Cortex's split function\n",
        "INSERT INTO CLAIM_PDF_CHUNKS (CLAIM_ID, RELATIVE_PATH, CHUNK)\n",
        "WITH text_chunks AS (\n",
        "    SELECT\n",
        "        CLAIM_ID,\n",
        "        RELATIVE_PATH,\n",
        "        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
        "            RAW_TEXT:content::STRING,  -- extract 'content' field\n",
        "            'markdown',                 -- tokenizer\n",
        "            800,                        -- chunk size\n",
        "            100,                        -- overlap\n",
        "            ARRAY_CONSTRUCT('\\n\\n')     -- preferred break\n",
        "        ) AS CHUNKS\n",
        "    FROM PARSED_CLAIM_PDFS\n",
        "    WHERE RAW_TEXT:content IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        "    CLAIM_ID,\n",
        "    RELATIVE_PATH,\n",
        "    chunk.value::STRING AS CHUNK\n",
        "FROM text_chunks,\n",
        "LATERAL FLATTEN(input => CHUNKS) AS chunk;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249d037c-8049-4b3e-9a25-cd8b3c92077d",
      "metadata": {
        "collapsed": false,
        "name": "CORTEX_SEARCH_SERVICES"
      },
      "source": [
        "### ðŸ” Make Your Transcripts Searchable with Cortex Search\n",
        "\n",
        "Now that we've extracted and chunked the content of each transcript, the next step is to make it **searchable** using Snowflake's fully managed retrieval system: **Cortex Search**.\n",
        "\n",
        "[`CORTEX SEARCH SERVICE`](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) enables **hybrid retrieval** â€” combining vector search (semantic meaning) with keyword search (lexical matching) â€” to deliver **highly accurate results** out of the box.\n",
        "\n",
        "This capability is a key enabler for Retrieval-Augmented Generation (RAG), allowing you to:\n",
        "- Ask contextual questions over your documents using SQL or LLM interfaces\n",
        "- Power chatbots and copilots with document-grounded responses\n",
        "- Serve precise and explainable enterprise search across large volumes of unstructured content\n",
        "\n",
        "ðŸ’¡ Powered by `snowflake-arctic-embed-l-v2.0`, a state-of-the-art embedding model optimized for enterprise search.\n",
        "\n",
        "Once the search service is defined, you can start querying it with natural language â€” directly inside Snowflake â€” without needing external pipelines, vector DBs, or hosting infrastructure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6cee862",
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Create a Cortex Search Service over your chunked transcripts table\n",
        "CREATE OR REPLACE CORTEX SEARCH SERVICE SNOWFLAKE_INTELLIGENCE.AGENTS.CLAIM_PDF_CHUNKS_SEARCH_SERVICE\n",
        "  ON CHUNK\n",
        "  ATTRIBUTES CLAIM_ID, RELATIVE_PATH\n",
        "  WAREHOUSE = SNOWFLAKE_INTELLIGENCE_WH\n",
        "  TARGET_LAG = '365 days'\n",
        "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
        "  AS (\n",
        "    SELECT\n",
        "      CLAIM_ID,\n",
        "      RELATIVE_PATH,\n",
        "      CHUNK::VARCHAR AS CHUNK\n",
        "    FROM SNOWFLAKE_INTELLIGENCE.CONFIG.CLAIM_PDF_CHUNKS\n",
        "  );"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0717015e",
      "metadata": {},
      "source": [
        "### Test your service â€” Baseline **â€œone-shotâ€ RAG** (even with a top-tier model)\n",
        "\n",
        "Select a customer and then run the *BaselineRAGPipeline* cell to perform the **simplest possible Retrieval-Augmented Generation flow**:\n",
        "\n",
        "1. **Retrieve** the **first** transcript chunk that semantically matches the userâ€™s question.  \n",
        "2. **Generate** an answer by stuffing that single chunk into the prompt of a **state-of-the-art model** (*`claude-3-5-sonnet`* or *`mistral-large2`*).\n",
        "\n",
        "---\n",
        "\n",
        "> Even with a premium LLM youâ€™ll notice it can only repeat whatever facts happen to live in that lone chunk.  \n",
        "> It frequently replies with **â€œBased on the limited context provided â€¦â€** because:\n",
        ">\n",
        "> * The relevant details might sit in a **different** chunk.  \n",
        "> * We pass **no metadata** (e.g., speaker tags, meeting header) to help the model reason.  \n",
        "> * We donâ€™t ask it to **extract** or **structure** anything.\n",
        ">\n",
        "> Youâ€™ll address all of these gaps in the next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5fc6f1",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Pull distinct CLAIM_IDs from your parsed PDF or chunks table\n",
        "claim_ids = [\n",
        "    r[\"CLAIM_ID\"]\n",
        "    for r in session.table(\"CLAIM_PDF_CHUNKS\")\n",
        "    .select(\"CLAIM_ID\")\n",
        "    .distinct()\n",
        "    .collect()\n",
        "]\n",
        "\n",
        "# Sort for a clean dropdown\n",
        "claim_ids = sorted(claim_ids)\n",
        "\n",
        "# User selects a claim ID to analyze\n",
        "selected = st.selectbox(\n",
        "    \"Run this cell, then select a claim ID to analyze\",\n",
        "    options=claim_ids,\n",
        "    key=\"selected_claim_id\"\n",
        ")\n",
        "\n",
        "st.success(f\"Chosen claim ID: {st.session_state.selected_claim_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c41098",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create service handle\n",
        "root = Root(get_active_session())\n",
        "claim_id = st.session_state.selected_claim_id\n",
        "svc  = root.databases[\"SNOWFLAKE_INTELLIGENCE\"]\\\n",
        "           .schemas[\"AGENTS\"]\\\n",
        "           .cortex_search_services[\"CLAIM_PDF_CHUNKS_SEARCH_SERVICE\"]  # --> Use your Snowflake Cortex Search Service\n",
        "\n",
        "# Specify user question\n",
        "question = f\"\"\"\n",
        "            Provide an explanation of benefits for claim ID {claim_id}.\n",
        "            Include diagnosis, procedure, amount billed, amount paid, patient responsibility,\n",
        "            and a short summary of the visit.\n",
        "            \"\"\".strip()\n",
        "\n",
        "# Get naive retrieval â€“ grab the very first hit (could be just the title page)\n",
        "hit = svc.search(\n",
        "        query            = question,\n",
        "        columns          = [\"CLAIM_ID\", \"CHUNK\", \"RELATIVE_PATH\"],\n",
        "        limit            = 5,                # look at a few chunks\n",
        "        search_type      = \"embed\"           # basic semantic search\n",
        "     ).results[0]\n",
        "\n",
        "st.info(f\"**File:** {hit['RELATIVE_PATH']}\\n\\n{hit['CHUNK']}\")\n",
        "\n",
        "# Single-pass generation\n",
        "response = Complete(\n",
        "             model  = \"claude-3-5-sonnet\",  # --> use Anthropic model or Mistral Large model\n",
        "             prompt = (\n",
        "                 f\"{question}\\n\\n\"\n",
        "                 \"Answer **only** using the context below:\\n\"\n",
        "                 \"-----\\n\"\n",
        "                 f\"{hit['CHUNK']}\\n\"\n",
        "                 \"-----\"\n",
        "             )\n",
        "           ).strip()\n",
        "\n",
        "st.info(f\"**LLM Response:**\\n\\n{response}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "lastEditStatus": {
      "authorEmail": "diana.shaw@snowflake.com",
      "authorId": "154296475017",
      "authorName": "DSHAW_SFC",
      "lastEditTime": 1747153634566,
      "notebookId": "uwskyl2ickjotzteiko6",
      "sessionId": "5acc5de6-8e3b-4693-ba67-0103c9ec8cba"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
